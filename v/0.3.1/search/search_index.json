{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"openEO - Concepts and API Reference Work in progress, please contribute by adding issues . openEO develops an open application programming interface(API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including accounting. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"Introduction"},{"location":"#openeo-concepts-and-api-reference","text":"Work in progress, please contribute by adding issues . openEO develops an open application programming interface(API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation Jointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts: Simplicity : nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows Unification : current EO cloud back-ends all have a different API , making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results, and manage user content including accounting. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"openEO - Concepts and API Reference"},{"location":"apireference-subscriptions/","text":"Placeholder for generated API specification for subscriptions.","title":"Subscriptions API Reference"},{"location":"apireference/","text":"Placeholder for generated API specification.","title":"Core API Reference"},{"location":"arch/","text":"Architecture The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture - openEO API shown in dark blue Microservices To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which datasets and image collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Job Management Organizes and manages jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Architecture"},{"location":"arch/#architecture","text":"The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order to group specific functionality. Figure: Architecture - openEO API shown in dark blue","title":"Architecture"},{"location":"arch/#microservices","text":"To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which datasets and image collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Discovery and execution of user-defined functions. Job Management Organizes and manages jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Secondary Services Management External web services to access data and job results such as a OGC WMTS service. Account Management User management, accounting and authentication.","title":"Microservices"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [0.3.1] - 2018-11-06 Added createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . Job ID in JSON response for GET /jobs/{job_id}/results . Changed Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now. Deprecated Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead. Fixed Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description . [0.3.0] - 2018-09-21 First version after proof of concept tackling many major issues. No changelog available. [0.0.2] - 2018-03-22 Version for proof of concept. No changelog available. [0.0.1] - 2018-02-07 Initial version.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#031-2018-11-06","text":"","title":"[0.3.1] - 2018-11-06"},{"location":"changelog/#added","text":"createProcessGraph method to client development guidelines. JSON file with all specified errors. Textual error codes for each specified error. Allow setting a plan for POST /preview Default billing plan in GET / . Job ID in JSON response for GET /jobs/{job_id}/results .","title":"Added"},{"location":"changelog/#changed","text":"Several optional fields such as output , title and description are now nullable instead of requiring to omit them. The output format is not required in POST /preview any more and thus allows falling back to the default. The output_format parameter in createJob and execute in client development guidelines. The extent parameters in filter_bbox and filter_daterange are formally required now.","title":"Changed"},{"location":"changelog/#deprecated","text":"Numeric openEO error codes are soon to be replaced with textual error codes. eo:resolution in collection bands is a duplicate of eo:gsd . Use eo:gsd instead.","title":"Deprecated"},{"location":"changelog/#fixed","text":"Fixed a wrong definition of the header OpenEO-Costs in POST /preview . Fixed typo in method authenticateOIDC in client development guidelines. Fixed the definition of spatial extents by swapping north and south. Replaced the outdated occurrences of srs with crs in spatial extents. Added missing required descriptions to process definitions. Added missing error messages. Fixed unclear specification for arrays used as process graph arguments. Fixed inconsist schema of openEO error responses: Field is now consistently named message instead of description .","title":"Fixed"},{"location":"changelog/#030-2018-09-21","text":"First version after proof of concept tackling many major issues. No changelog available.","title":"[0.3.0] - 2018-09-21"},{"location":"changelog/#002-2018-03-22","text":"Version for proof of concept. No changelog available.","title":"[0.0.2] - 2018-03-22"},{"location":"changelog/#001-2018-02-07","text":"Initial version.","title":"[0.0.1] - 2018-02-07"},{"location":"codeofconduct/","text":"Contributor Code of Conduct As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"codeofconduct/#contributor-code-of-conduct","text":"As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"collections/","text":"Data Discovery (Collections) openEO strives for compatibility with STAC and OGC WFS 3.0 as far as possible. Implementing the data discovery endpoints of openEO should also produce mostly valid STAC and WFS 3.0 responses, including an incomplete compatibility with their APIs (see below). Warning STAC and OGC WFS 3.0, as well as openEO, are still under development and openEO currently only integrates intermediate dev versions due to time constraints. Therefore, it is very likely that further changes and adjustments will be made in the future. Extensions STAC has several extensions (see their repository ) that can be used to better describe your data. Clients and server are not required to implement all of them, so be aware that some clients may not be able to read all your meta data. Some commonly used extensions are: EO extension (mostly integrated within openEO) Scientific extension (integrated within openEO) Dimensions extension (draft) Links For data discovery in general and each collection you can specify a set of references. These can be alternate representations, e.g. data discovery via OGC WCS or OGC CSW, references to a license, references to actual raw data for downloading, detailed information about pre-processing, etc. Note STAC requires to add a link with relation type self (see below). Although this is not technically necessary for openEO and we do not enforce you with our validation tools to provide such a link, we still recommend to provide it anyway for compatibility reasons. Common link relation types The following table lists relation types that are commonly used as rel types in the links. The scope 'Collections' refers to the links that are related to a specific collection, 'Discovery' refers to links that are related to data discovery in general and are not about a specific collection. Type Description Scope self Absolute URL to the data discovery endpoint or the collection itself. Discovery +Collections root / parent URL to the data discovery endpoint. Collections child URL to a child STAC Catalog or STAC Dataset. Collections item URL to a STAC Item. Collections license The license URL for the dataset SHOULD be specified if the license field is set to proprietary . If there is no public license URL available, it is RECOMMENDED to supplement the collection with the license text in a separate file and link to this file. Collections alternate An alternative representation of the metadata. This could be a secondary web service such as OGC WCS or OGC CSW or a metadata document following another standard such as ISO 19115, INSPIRE or DCAT. Discovery +Collections about A resource that is related or further explains the entity, e.g. a user guide. Discovery +Collections derived_from Allows referencing the data this collection was derived from. Collections cite-as For all DOI names specified, the respective DOI links SHOULD be added to the links section of the catalog with the rel type cite-as . Collections More relation types may be listed in the STAC documentation. Compatibility with WFS and STAC APIs The data discovery endpoints GET /collections and GET /collections/{name} are compatible with WFS 3.0 and STAC. The only limitation with regard to response compatibility is that openEO allows open date ranges and WFS does not (see issue WFS_FES#155 ). Additionally, STAC and WFS define additional endpoints that need to be implemented to be fully compatible. The additional information can easily be integrated into an openEO API implementation. A rough list of actions for compatibility is available below, but please refer to their specifications to find out the full details. WFS 3.0 compatibility As of now, WFS 3.0 requires more endpoints for full compatibility. You should make the following changes to your API to implement a valid WFS: Add a links property to the GET / request that links to the WFS endpoints. Implement GET /api and return the WFS OpenAPI document. Implement GET /conformance and specify which conformance classes your WFS conforms to. Implement GET /collections/{collection-name}/items and GET /collections/{collection-name}/items/{feature-id} to support retrieval of individual features. STAC compatibility As of now, STAC has two more required endpoints that need to be implemented: GET /stac POST /stac/search","title":"Data Discovery"},{"location":"collections/#data-discovery-collections","text":"openEO strives for compatibility with STAC and OGC WFS 3.0 as far as possible. Implementing the data discovery endpoints of openEO should also produce mostly valid STAC and WFS 3.0 responses, including an incomplete compatibility with their APIs (see below). Warning STAC and OGC WFS 3.0, as well as openEO, are still under development and openEO currently only integrates intermediate dev versions due to time constraints. Therefore, it is very likely that further changes and adjustments will be made in the future.","title":"Data Discovery (Collections)"},{"location":"collections/#extensions","text":"STAC has several extensions (see their repository ) that can be used to better describe your data. Clients and server are not required to implement all of them, so be aware that some clients may not be able to read all your meta data. Some commonly used extensions are: EO extension (mostly integrated within openEO) Scientific extension (integrated within openEO) Dimensions extension (draft)","title":"Extensions"},{"location":"collections/#links","text":"For data discovery in general and each collection you can specify a set of references. These can be alternate representations, e.g. data discovery via OGC WCS or OGC CSW, references to a license, references to actual raw data for downloading, detailed information about pre-processing, etc. Note STAC requires to add a link with relation type self (see below). Although this is not technically necessary for openEO and we do not enforce you with our validation tools to provide such a link, we still recommend to provide it anyway for compatibility reasons.","title":"Links"},{"location":"collections/#common-link-relation-types","text":"The following table lists relation types that are commonly used as rel types in the links. The scope 'Collections' refers to the links that are related to a specific collection, 'Discovery' refers to links that are related to data discovery in general and are not about a specific collection. Type Description Scope self Absolute URL to the data discovery endpoint or the collection itself. Discovery +Collections root / parent URL to the data discovery endpoint. Collections child URL to a child STAC Catalog or STAC Dataset. Collections item URL to a STAC Item. Collections license The license URL for the dataset SHOULD be specified if the license field is set to proprietary . If there is no public license URL available, it is RECOMMENDED to supplement the collection with the license text in a separate file and link to this file. Collections alternate An alternative representation of the metadata. This could be a secondary web service such as OGC WCS or OGC CSW or a metadata document following another standard such as ISO 19115, INSPIRE or DCAT. Discovery +Collections about A resource that is related or further explains the entity, e.g. a user guide. Discovery +Collections derived_from Allows referencing the data this collection was derived from. Collections cite-as For all DOI names specified, the respective DOI links SHOULD be added to the links section of the catalog with the rel type cite-as . Collections More relation types may be listed in the STAC documentation.","title":"Common link relation types"},{"location":"collections/#compatibility-with-wfs-and-stac-apis","text":"The data discovery endpoints GET /collections and GET /collections/{name} are compatible with WFS 3.0 and STAC. The only limitation with regard to response compatibility is that openEO allows open date ranges and WFS does not (see issue WFS_FES#155 ). Additionally, STAC and WFS define additional endpoints that need to be implemented to be fully compatible. The additional information can easily be integrated into an openEO API implementation. A rough list of actions for compatibility is available below, but please refer to their specifications to find out the full details.","title":"Compatibility with WFS and STAC APIs"},{"location":"collections/#wfs-30-compatibility","text":"As of now, WFS 3.0 requires more endpoints for full compatibility. You should make the following changes to your API to implement a valid WFS: Add a links property to the GET / request that links to the WFS endpoints. Implement GET /api and return the WFS OpenAPI document. Implement GET /conformance and specify which conformance classes your WFS conforms to. Implement GET /collections/{collection-name}/items and GET /collections/{collection-name}/items/{feature-id} to support retrieval of individual features.","title":"WFS 3.0 compatibility"},{"location":"collections/#stac-compatibility","text":"As of now, STAC has two more required endpoints that need to be implemented: GET /stac POST /stac/search","title":"STAC compatibility"},{"location":"cors/","text":"Cross-Origin Resource Sharing (CORS) Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support. 1. Supporting the OPTIONS method All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json Example request and response Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json 2. Sending CORS headers for every endpoint The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Tip Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"CORS"},{"location":"cors/#cross-origin-resource-sharing-cors","text":"Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support.","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"cors/#1-supporting-the-options-method","text":"All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json","title":"1. Supporting the OPTIONS method"},{"location":"cors/#example-request-and-response","text":"Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json","title":"Example request and response"},{"location":"cors/#2-sending-cors-headers-for-every-endpoint","text":"The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Tip Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"2. Sending CORS headers for every endpoint"},{"location":"errors/","text":"Status and error handling The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors. JSON error object A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : 123 , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/123\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized openEO error codes below or a proprietary error code. Please note, that the numeric error code will be replaced by a descriptive textual code such as SampleError in API v0.4.0. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Accept-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth. Standardized status codes The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. 403 Forbidden : The client did provided correct authentication details, but the privileges/permissions of the provided credentials do not allow to request the resource. 404 Not Found : The resource specified by the path does not exist, i.e. one of the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code. openEO error codes The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses. Categories Account Management EO Data Discovery File Management General Job Management Process Graph Management Processes Secondary Services Management Subscriptions Account Management openEO Error Code openEO Error Name Description Message HTTP Status Code 401 AuthenticationRequired The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. Unauthorized. 401 4031 AuthenticationSchemeInvalid Invalid authentication scheme (e.g. Bearer). Authentication method not supported. 403 4033 CredentialsInvalid Credentials are not correct. 403 4032 TokenInalid Authorization token invalid or expired. Session has expired. 403 EO Data Discovery openEO Error Code openEO Error Name Description Message HTTP Status Code 700 CollectionNotFound The requested collection does not exist. Collection does not exist. 404 File Management openEO Error Code openEO Error Name Description Message HTTP Status Code 1409 ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. Media type specified in the request is not supported. Supported media types: {types} 400 1412 FileContentInvalid The content of the file is invalid. File content is invalid. 400 1413 FileLocked The file is locked by a running job or another process. File '{file}' is locked. 400 1400 FileNotFound The requested file does not exist. File does not exist. 404 1404 FileOperationUnsupported The specified path is not a file and the operation is only supported for files. Path is likely a directory. Operation is only supported for files. 400 1403 FilePathInvalid The specified path is invalid or not accessible. Path could contain invalid characters, an invalid user ID or point to an existing folder or a location outside of the user folder. File path is invalid. 400 1411 FileSizeExceeded File exceeds allowed maximum file size. File size it too large. Maximum file size: {size} 400 1410 FileTypeInvalid File format, file extension or media (MIME) type is not allowed. File type not allowed. Allowed file types: {types} 400 1401 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 1402 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 General openEO Error Code openEO Error Name Description Message HTTP Status Code 1409 ContentTypeInvalid The specified media (MIME) type used in the Content-Type header is not allowed. Media type specified in the request is not supported. Supported media types: {types} 400 501 FeatureUnsupported The back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported. Feature not supported. 501 603 InfrastructureBusy Service is generally available, but the infrastructure can't handle it at the moment as too many requests are processed. Service is not available at the moment due to overloading. Please try again later. 503 503 InfrastructureMaintenance Service is currently not available, but the infrastructure is currently undergoing maintenance work. Service is not available at the moment due to maintenance work. Please try again later. 503 500 Internal An internal server error with a proprietary message. Server error: {message} 500 404 NotFound To be used if the requested resource does not exist. Note: Unsupported endpoints MUST send an 'FeatureUnsupported' error. There are also specialized errors for missing jobs (JobNotFound), files (FileNotFound), etc. Resource not found. 404 408 Timeout The request took too long and timed out. Request timed out. 408 Job Management openEO Error Code openEO Error Name Description Message HTTP Status Code 4101 BillingPlanInvalid The specified billing plan is not on the list of available plans. The specified billing plan is not valid. 400 621 BudgetInvalid The specified budget is too low as it is either smaller than or equal to 0 or below the costs. The specified budget is too low. 400 3003 FormatArgumentInvalid The value specified for the output format argument '{argument}' is invalid: {reason} 400 3002 FormatArgumentUnsupported Output format argument '{argument}' is not supported. 400 3004 FormatUnsuitable Data can't be transformed into the requested output format. 400 3001 FormatUnsupported Output format not supported. 400 3005 JobLocked The job is currently locked due to a running batch computation and can't be modified meanwhile. Job is locked due to a queued or running batch computation. 400 3006 JobNotFinished Job has not finished computing the results yet. Please try again later. 400 5101 JobNotFound The requested job does not exist. The job does not exist. 404 3007 JobNotStarted Job has not been queued or started yet or was canceled and not restarted by the user. Job hasn't been started yet. 400 631 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 402 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 2001 ProcessGraphMissing No valid process graph specified. 400 632 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 1401 StorageFailure Server couldn't store file(s) due to server-side reasons. Unable to store file(s). 500 1402 StorageQuotaExceeded The storage quota has been exceeded by the user. Insufficient Storage. 400 408 Timeout The request took too long and timed out. Request timed out. 408 2004 VariableDefaultValueTypeInvalid The default value specified for the process graph variable '{variable_id}' is not of type '{type}'. 400 2005 VariableIdInvalid A specified variable ID is not valid. 400 2006 VariableTypeInvalid The data type specified for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 2004 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Process Graph Management openEO Error Code openEO Error Name Description Message HTTP Status Code 631 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 2001 ProcessGraphMissing No valid process graph specified. 400 2000 ProcessGraphNotFound The requested process graph does not exist. Process graph does not exist. 404 632 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 2004 VariableDefaultValueTypeInvalid The default value specified for the process graph variable '{variable_id}' is not of type '{type}'. 400 2005 VariableIdInvalid A specified variable ID is not valid. 400 2006 VariableTypeInvalid The data type specified for the process graph variable '{variable_id}' is invalid. Must be one of: string, boolean, number, array or object. 400 2004 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Processes openEO Error Code openEO Error Name Description Message HTTP Status Code 611 CRSInvalid Invalid or unsupported CRS specified. CRS '{crs}' is invalid. 400 700 CollectionNotFound The requested collection does not exist. Collection does not exist. 404 612 CoordinateOutOfBounds Coordinate is out of bounds. 400 1412 FileContentInvalid The content of the file is invalid. File content is invalid. 400 1400 FileNotFound The requested file does not exist. File does not exist. 404 5101 JobNotFound The requested job does not exist. The job does not exist. 404 2103 ProcessArgumentInvalid The value specified for the process argument '{argument}' in process '{process}' is invalid: {reason} 400 2104 ProcessArgumentRequired Process '{process}' requires argument '{argument}'. 400 2102 ProcessArgumentUnsupported Process '{process}' does not support argument '{argument}'. 400 2105 ProcessArgumentsMissing Process '{process}' requires at least '{min_parameters}' parameters. 400 2101 ProcessUnsupported Process '{process}' is not supported. 400 Secondary Services Management openEO Error Code openEO Error Name Description Message HTTP Status Code 4101 BillingPlanInvalid The specified billing plan is not on the list of available plans. The specified billing plan is not valid. 400 621 BudgetInvalid The specified budget is too low as it is either smaller than or equal to 0 or below the costs. The specified budget is too low. 400 631 NoDataForUpdate For PATCH requests: No valid data specified at all. No valid data specified to be updated. 400 402 PaymentRequired The budget required to fulfil the request are insufficient. Payment required. 402 2001 ProcessGraphMissing No valid process graph specified. 400 632 PropertyNotEditable For PATCH requests: The specified parameter can't be updated. It is read-only. Specified property '{property}' is read-only. 400 5103 ServiceArgumentInvalid The value specified for the secondary service argument '{argument}' is invalid: {reason} 400 5104 ServiceArgumentRequired Required secondary service argument '{argument}' is missing. 400 5102 ServiceArgumentUnsupported Secondary service argument '{argument}' is not supported. 400 5000 ServiceNotFound The requested secondary service does not exist. Service does not exist. 404 5001 ServiceUnsupported Secondary service type is not supported. 400 2004 VariableValueMissing No value specified for process graph variable '{variable_id}'. 400 Subscriptions openEO Error Code openEO Error Name Description Message HTTP Status Code 801 WebSocketUpgradeNotRequested In order to subscribe the connection must be upgradable to WebSockets. Client sent invalid request to establish subscriptions. 400","title":"Error Handling"},{"location":"errors/#status-and-error-handling","text":"The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors.","title":"Status and error handling"},{"location":"errors/#json-error-object","text":"A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : 123 , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/123\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized openEO error codes below or a proprietary error code. Please note, that the numeric error code will be replaced by a descriptive textual code such as SampleError in API v0.4.0. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Accept-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.","title":"JSON error object"},{"location":"errors/#standardized-status-codes","text":"The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authentication details for a resource requiring authentication or the provided authentication details are not correct. 403 Forbidden : The client did provided correct authentication details, but the privileges/permissions of the provided credentials do not allow to request the resource. 404 Not Found : The resource specified by the path does not exist, i.e. one of the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.","title":"Standardized status codes"},{"location":"errors/#openeo-error-codes","text":"The following table of error codes is incomplete . These error codes will evolve over time. If you are missing any common error, please contribute it by adding an issue , creating a pull request or get in contact in our chat room . The whole table of error codes is available as JSON file , which can be used by implementors to automatically generate error responses.","title":"openEO error codes"},{"location":"examples-poc/","text":"Examples (proof of concept) This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons Note CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests for each use case individually. Use Case 1 Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. 1. Requesting the capabilities of the back-end Request GET / Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"version\" : \"0.3.1\" , \"endpoints\" : [ { \"path\" : \"/collections\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/collections/{name}\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/processes\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/jobs\" , \"methods\" : [ \"GET\" , \"POST\" ] }, { \"path\" : \"/jobs/{job_id}\" , \"methods\" : [ \"GET\" , \"DELETE\" , \"PATCH\" ] }, { \"path\" : \"/jobs/{job_id}/results\" , \"methods\" : [ \"GET\" , \"POST\" , \"DELETE\" ] }, { \"path\" : \"/services\" , \"methods\" : [ \"GET\" , \"POST\" ] }, { \"path\" : \"/services/{service_id}\" , \"methods\" : [ \"GET\" , \"DELETE\" , \"PATCH\" ] } ], \"billing\" : { \"currency\" : \"EUR\" , \"plans\" : [ { \"name\" : \"free\" , \"description\" : \"Free plan. Calculates one tile per second and a maximum amount of 100 tiles per hour.\" , \"url\" : \"http://openeo.org/plans/free-plan\" } ] } } 2. Check whether Sentinel 2A Level 1C data is available at the back-end Request GET /collections HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"collections\" : [ { \"name\" : \"Sentinel-2A\" , \"title\" : \"Sentinel-2A MSI L1C\" , \"description\" : \"Sentinel-2A is a wide-swath, high-resolution, multi-spectral imaging mission supporting Copernicus Land Monitoring studies, including the monitoring of vegetation, soil and water cover, as well as observation of inland waterways and coastal areas.\" , \"license\" : \"proprietary\" , \"extent\" : { \"spatial\" : [ 180 , -56 , -180 , 83 ], \"temporal\" : [ \"2015-06-23T00:00:00Z\" , null ] }, \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections/Sentinel-2A\" }, { \"rel\" : \"license\" , \"href\" : \"https://scihub.copernicus.eu/twiki/pub/SciHubWebPortal/TermsConditions/Sentinel_Data_Terms_and_Conditions.pdf\" } ] } ], \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections\" } ] } 3. Check that needed processes are available Request GET /processes HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"processes\" : [ { \"name\" : \"get_collection\" , \"summary\" : \"Selects a collection.\" , \"description\" : \"Filters and selects a single collection provided by the back-end. The back-end provider decides which of the potential collections is the most relevant one to be selected.\" , \"min_parameters\" : 1 , \"parameters\" : { \"name\" : { \"description\" : \"Filter by collection name\" , \"schema\" : { \"type\" : \"string\" , \"examples\" : [ \"Sentinel2A-L1C\" ] } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_bands\" , \"summary\" : \"Filters by bands.\" , \"description\" : \"Allows to extract one or multiple bands of multi-band raster image collection.\\nBands can be chosen either by band id, band name or by wavelength.\\n\\nimagery and at one of the other arguments is required to be specified.\" , \"min_parameters\" : 2 , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"bands\" : { \"description\" : \"string or array of strings containing band ids.\" , \"schema\" : { \"type\" : [ \"string\" , \"array\" ], \"items\" : { \"type\" : \"string\" } } }, \"names\" : { \"description\" : \"string or array of strings containing band names.\" , \"schema\" : { \"type\" : [ \"string\" , \"array\" ], \"items\" : { \"type\" : \"string\" } } }, \"wavelengths\" : { \"description\" : \"Either a number specifying a specific wavelength or a two-element array of numbers specifying a minimum and maximum wavelength.\" , \"schema\" : { \"type\" : [ \"number\" , \"array\" ], \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"number\" } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_daterange\" , \"summary\" : \"Filters by temporal extent.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"extent\" : { \"description\" : \"Temporal extent specified by a start and an end time, each formatted as a [RFC 3339](https://www.ietf.org/rfc/rfc3339) date-time. Open date ranges are supported and can be specified by setting one of the times to null. Setting both entries to null is not allowed.\" , \"schema\" : { \"type\" : \"array\" , \"format\" : \"temporal_extent\" , \"required\" : true , \"example\" : [ \"2016-01-01T00:00:00Z\" , \"2017-10-01T00:00:00Z\" ], \"items\" : { \"type\" : [ \"string\" , \"null\" ], \"format\" : \"date-time\" , \"minItems\" : 2 , \"maxItems\" : 2 } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_bbox\" , \"summary\" : \"Filters by spatial extent.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"extent\" : { \"description\" : \"Spatial extent, may include a vertical axis (height or depth).\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"spatial_extent\" , \"required\" : [ \"west\" , \"south\" , \"east\" , \"north\" ], \"properties\" : { \"crs\" : { \"description\" : \"Coordinate reference system. EPSG codes must be supported. In addition, proj4 strings should be supported by back-ends. Whenever possible, it is recommended to use EPSG codes instead of proj4 strings.\\nDefaults to `EPSG:4326` unless the client explicitly requests a different coordinate reference system.\" , \"type\" : \"string\" , \"default\" : \"EPSG:4326\" }, \"west\" : { \"description\" : \"West (lower left corner, coordinate axis 1).\" , \"type\" : \"number\" }, \"south\" : { \"description\" : \"South (lower left corner, coordinate axis 2).\" , \"type\" : \"number\" }, \"east\" : { \"description\" : \"East (upper right corner, coordinate axis 1).\" , \"type\" : \"number\" }, \"north\" : { \"description\" : \"North (upper right corner, coordinate axis 2).\" , \"type\" : \"number\" }, \"base\" : { \"description\" : \"Base (optional, lower left corner, coordinate axis 3).\" , \"type\" : \"number\" }, \"height\" : { \"description\" : \"Height (optional, upper right corner, coordinate axis 3).\" , \"type\" : \"number\" } } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"NDVI\" , \"summary\" : \"Calculates the Normalized Difference Vegetation Index.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"red\" : { \"description\" : \"Band id of the red band.\" , \"required\" : true , \"schema\" : { \"type\" : \"string\" } }, \"nir\" : { \"description\" : \"Band id of the near-infrared band.\" , \"required\" : true , \"schema\" : { \"type\" : \"string\" } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"exceptions\" : { \"RedBandInvalid\" : { \"description\" : \"The specified red band is not available or contains invalid data.\" }, \"NirBandInvalid\" : { \"description\" : \"The specified nir band is not available or contains invalid data.\" } } }, { \"name\" : \"min_time\" , \"summary\" : \"Calculates minimum values of time series.\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } } ], \"links\" : {} } 4. Request the supported secondary web service types Request GET /service_types HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"WMS\" : { \"parameters\" : { \"version\" : { \"type\" : \"string\" , \"description\" : \"The WMS version to use.\" , \"default\" : \"1.3.0\" , \"enum\" : [ \"1.1.1\" , \"1.3.0\" ] } }, \"attributes\" : { \"layers\" : { \"type\" : \"array\" , \"description\" : \"Array of layer names.\" , \"example\" : [ \"roads\" , \"countries\" , \"water_bodies\" ] } } } } 5. Create a WMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"process_id\" : \"min_time\" , \"imagery\" : { \"process_id\" : \"NDVI\" , \"imagery\" : { \"process_id\" : \"filter_daterange\" , \"imagery\" : { \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"extent\" : [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B4\" , \"nir\" : \"B8\" } }, \"type\" : \"WMS\" , \"enabled\" : true , \"parameters\" : { \"version\" : \"1.1.1\" }, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9 6. Requesting the service information Request POST https://openeo.org/api/v0.3/services/wms-a3cca9 HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"service_id\" : \"wms-a3cca9\" , \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"process_id\" : \"min_time\" , \"imagery\" : { \"process_id\" : \"NDVI\" , \"imagery\" : { \"process_id\" : \"filter_daterange\" , \"imagery\" : { \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"extent\" : [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B4\" , \"nir\" : \"B8\" } }, \"url\" : \"https://openeo.org/wms/a3cca9\" , \"type\" : \"WMS\" , \"enabled\" : true , \"parameters\" : { \"version\" : \"1.1.1\" }, \"attributes\" : { \"layers\" : [ \"min_time\" ] }, \"submitted\" : \"2017-01-01T09:32:12Z\" , \"plan\" : \"free\" , \"budget\" : null } 7. Download the data on demand from the WMS Omitted, not part of the openEO API. WMS is located at https://openeo.org/wms/a3cca9 . Use Case 2 Create a monthly aggregated Sentinel 1 product from a custom Python script. 1. Ask the back-end for available Sentinel 1 data Request GET /collections/Sentinel-1 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"name\" : \"Sentinel-1\" , \"description\" : \"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\" , \"license\" : \"proprietary\" , \"keywords\" : [ \"copernicus\" , \"esa\" , \"sentinel\" , \"sar\" ], \"provider\" : [ { \"name\" : \"European Space Agency (ESA)\" , \"url\" : \"https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar\" } ], \"extent\" : { \"spatial\" : [ -34 , 35 , 39 , 71 ], \"temporal\" : [ \"2016-01-01T00:00:00Z\" , null ] }, \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections/Sentinel-1\" }, { \"rel\" : \"license\" , \"href\" : \"https://scihub.copernicus.eu/twiki/pub/SciHubWebPortal/TermsConditions/Sentinel_Data_Terms_and_Conditions.pdf\" } ], \"eo:constellation\" : \"sentinel-1\" , \"eo:bands\" : { \"VV\" : { \"common_name\" : \"VV\" }, \"VH\" : { \"common_name\" : \"VH\" } } } 2. Upload python script Request PUT /files/john_doe/s1_aggregate.py HTTP / 1.1 <File content> Response HTTP / 1.1 204 No Content 3. Create a job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" :{ \"process_id\" : \"aggregate_time\" , \"script\" : \"/files/john_doe/s1_aggregate.py\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-1\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] } }, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/jobs/132 OpenEO-Identifier : 132 4. Start batch processing the job Request POST /jobs/132/results HTTP / 1.1 Response HTTP / 1.1 202 Accepted 5. Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"process_id\" : \"get_results\" , \"job_id\" : \"132\" }, \"type\" : \"TMS\" , \"enabled\" : true , \"parameters\" : {}, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/services/tms-75ff8c 6. Requesting the service information Request POST https://openeo.org/api/v0.3/services/tms-75ff8c HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"service_id\" : \"tms-75ff8c\" , \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"process_id\" : \"get_results\" , \"job_id\" : \"132\" }, \"url\" : \"https://openeo.org/tms/75ff8c\" , \"type\" : \"TMS\" , \"enabled\" : true , \"parameters\" : {}, \"attributes\" : {}, \"submitted\" : \"2018-01-01T12:32:12Z\" , \"plan\" : \"free\" , \"budget\" : null } 7. Download the data on demand from the WMS Omitted, not part of the openEO API. TMS is located at https://openeo.org/tms/75ff8c . Use Case 3 Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons. 1. Upload a GeoJSON Polygon Request PUT /files/john_doe/polygon1.geojson HTTP / 1.1 <File content> Response HTTP / 1.1 204 No Content 2. Create a job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"bands\" : \"B8\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"regions\" : \"/files/john_doe/polygon1.geojson\" , \"func\" : \"mean\" }, \"output\" :{ \"format\" : \"GPKG\" }, \"plan\" : \"free\" , \"budget\" : null } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133 3. Start batch processing the job Request POST /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 202 Accepted 4. Check job status Request GET /jobs/133 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"job_id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"bands\" : \"B8\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"regions\" : \"/files/john_doe/polygon1.geojson\" , \"func\" : \"mean\" }, \"output\" :{ \"format\" : \"GPKG\" }, \"status\" : \"finished\" , \"submitted\" : \"2017-02-01T09:32:12Z\" , \"updated\" : \"2017-02-01T09:36:18Z\" , \"plan\" : \"free\" , \"costs\" : 0 , \"budget\" : null } 5. Retrieve download links Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"updated\" : \"2017-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.gpkg\" , \"type\" : \"application/geopackage+sqlite3\" } ] } 6. Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.gpkg HTTP / 1.1 Response A GeoPackage file, content omitted.","title":"Examples (proof of concept)"},{"location":"examples-poc/#examples-proof-of-concept","text":"This page gives a detailed description of the openEO proof of concept use cases. After the proof of concept, this stays in the API to have some basic examples. The proof of concept covered three clearly defined example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons Note CORS and authentication is not included in these examples for simplicity. Repeating calls are also not included as it would not make much sense to list the same discovery requests for each use case individually.","title":"Examples (proof of concept)"},{"location":"examples-poc/#use-case-1","text":"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.","title":"Use Case 1"},{"location":"examples-poc/#1-requesting-the-capabilities-of-the-back-end","text":"Request GET / Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"version\" : \"0.3.1\" , \"endpoints\" : [ { \"path\" : \"/collections\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/collections/{name}\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/processes\" , \"methods\" : [ \"GET\" ] }, { \"path\" : \"/jobs\" , \"methods\" : [ \"GET\" , \"POST\" ] }, { \"path\" : \"/jobs/{job_id}\" , \"methods\" : [ \"GET\" , \"DELETE\" , \"PATCH\" ] }, { \"path\" : \"/jobs/{job_id}/results\" , \"methods\" : [ \"GET\" , \"POST\" , \"DELETE\" ] }, { \"path\" : \"/services\" , \"methods\" : [ \"GET\" , \"POST\" ] }, { \"path\" : \"/services/{service_id}\" , \"methods\" : [ \"GET\" , \"DELETE\" , \"PATCH\" ] } ], \"billing\" : { \"currency\" : \"EUR\" , \"plans\" : [ { \"name\" : \"free\" , \"description\" : \"Free plan. Calculates one tile per second and a maximum amount of 100 tiles per hour.\" , \"url\" : \"http://openeo.org/plans/free-plan\" } ] } }","title":"1. Requesting the capabilities of the back-end"},{"location":"examples-poc/#2-check-whether-sentinel-2a-level-1c-data-is-available-at-the-back-end","text":"Request GET /collections HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"collections\" : [ { \"name\" : \"Sentinel-2A\" , \"title\" : \"Sentinel-2A MSI L1C\" , \"description\" : \"Sentinel-2A is a wide-swath, high-resolution, multi-spectral imaging mission supporting Copernicus Land Monitoring studies, including the monitoring of vegetation, soil and water cover, as well as observation of inland waterways and coastal areas.\" , \"license\" : \"proprietary\" , \"extent\" : { \"spatial\" : [ 180 , -56 , -180 , 83 ], \"temporal\" : [ \"2015-06-23T00:00:00Z\" , null ] }, \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections/Sentinel-2A\" }, { \"rel\" : \"license\" , \"href\" : \"https://scihub.copernicus.eu/twiki/pub/SciHubWebPortal/TermsConditions/Sentinel_Data_Terms_and_Conditions.pdf\" } ] } ], \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections\" } ] }","title":"2. Check whether Sentinel 2A Level 1C data is available at the back-end"},{"location":"examples-poc/#3-check-that-needed-processes-are-available","text":"Request GET /processes HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"processes\" : [ { \"name\" : \"get_collection\" , \"summary\" : \"Selects a collection.\" , \"description\" : \"Filters and selects a single collection provided by the back-end. The back-end provider decides which of the potential collections is the most relevant one to be selected.\" , \"min_parameters\" : 1 , \"parameters\" : { \"name\" : { \"description\" : \"Filter by collection name\" , \"schema\" : { \"type\" : \"string\" , \"examples\" : [ \"Sentinel2A-L1C\" ] } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_bands\" , \"summary\" : \"Filters by bands.\" , \"description\" : \"Allows to extract one or multiple bands of multi-band raster image collection.\\nBands can be chosen either by band id, band name or by wavelength.\\n\\nimagery and at one of the other arguments is required to be specified.\" , \"min_parameters\" : 2 , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"bands\" : { \"description\" : \"string or array of strings containing band ids.\" , \"schema\" : { \"type\" : [ \"string\" , \"array\" ], \"items\" : { \"type\" : \"string\" } } }, \"names\" : { \"description\" : \"string or array of strings containing band names.\" , \"schema\" : { \"type\" : [ \"string\" , \"array\" ], \"items\" : { \"type\" : \"string\" } } }, \"wavelengths\" : { \"description\" : \"Either a number specifying a specific wavelength or a two-element array of numbers specifying a minimum and maximum wavelength.\" , \"schema\" : { \"type\" : [ \"number\" , \"array\" ], \"minItems\" : 2 , \"maxItems\" : 2 , \"items\" : { \"type\" : \"number\" } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_daterange\" , \"summary\" : \"Filters by temporal extent.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"extent\" : { \"description\" : \"Temporal extent specified by a start and an end time, each formatted as a [RFC 3339](https://www.ietf.org/rfc/rfc3339) date-time. Open date ranges are supported and can be specified by setting one of the times to null. Setting both entries to null is not allowed.\" , \"schema\" : { \"type\" : \"array\" , \"format\" : \"temporal_extent\" , \"required\" : true , \"example\" : [ \"2016-01-01T00:00:00Z\" , \"2017-10-01T00:00:00Z\" ], \"items\" : { \"type\" : [ \"string\" , \"null\" ], \"format\" : \"date-time\" , \"minItems\" : 2 , \"maxItems\" : 2 } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"filter_bbox\" , \"summary\" : \"Filters by spatial extent.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"extent\" : { \"description\" : \"Spatial extent, may include a vertical axis (height or depth).\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"spatial_extent\" , \"required\" : [ \"west\" , \"south\" , \"east\" , \"north\" ], \"properties\" : { \"crs\" : { \"description\" : \"Coordinate reference system. EPSG codes must be supported. In addition, proj4 strings should be supported by back-ends. Whenever possible, it is recommended to use EPSG codes instead of proj4 strings.\\nDefaults to `EPSG:4326` unless the client explicitly requests a different coordinate reference system.\" , \"type\" : \"string\" , \"default\" : \"EPSG:4326\" }, \"west\" : { \"description\" : \"West (lower left corner, coordinate axis 1).\" , \"type\" : \"number\" }, \"south\" : { \"description\" : \"South (lower left corner, coordinate axis 2).\" , \"type\" : \"number\" }, \"east\" : { \"description\" : \"East (upper right corner, coordinate axis 1).\" , \"type\" : \"number\" }, \"north\" : { \"description\" : \"North (upper right corner, coordinate axis 2).\" , \"type\" : \"number\" }, \"base\" : { \"description\" : \"Base (optional, lower left corner, coordinate axis 3).\" , \"type\" : \"number\" }, \"height\" : { \"description\" : \"Height (optional, upper right corner, coordinate axis 3).\" , \"type\" : \"number\" } } } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, { \"name\" : \"NDVI\" , \"summary\" : \"Calculates the Normalized Difference Vegetation Index.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"red\" : { \"description\" : \"Band id of the red band.\" , \"required\" : true , \"schema\" : { \"type\" : \"string\" } }, \"nir\" : { \"description\" : \"Band id of the near-infrared band.\" , \"required\" : true , \"schema\" : { \"type\" : \"string\" } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } }, \"exceptions\" : { \"RedBandInvalid\" : { \"description\" : \"The specified red band is not available or contains invalid data.\" }, \"NirBandInvalid\" : { \"description\" : \"The specified nir band is not available or contains invalid data.\" } } }, { \"name\" : \"min_time\" , \"summary\" : \"Calculates minimum values of time series.\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"parameters\" : { \"imagery\" : { \"description\" : \"EO data to process.\" , \"required\" : true , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } }, \"returns\" : { \"description\" : \"Processed EO data.\" , \"schema\" : { \"type\" : \"object\" , \"format\" : \"eodata\" } } } ], \"links\" : {} }","title":"3. Check that needed processes are available"},{"location":"examples-poc/#4-request-the-supported-secondary-web-service-types","text":"Request GET /service_types HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"WMS\" : { \"parameters\" : { \"version\" : { \"type\" : \"string\" , \"description\" : \"The WMS version to use.\" , \"default\" : \"1.3.0\" , \"enum\" : [ \"1.1.1\" , \"1.3.0\" ] } }, \"attributes\" : { \"layers\" : { \"type\" : \"array\" , \"description\" : \"Array of layer names.\" , \"example\" : [ \"roads\" , \"countries\" , \"water_bodies\" ] } } } }","title":"4. Request the supported secondary web service types"},{"location":"examples-poc/#5-create-a-wms-service","text":"Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"process_id\" : \"min_time\" , \"imagery\" : { \"process_id\" : \"NDVI\" , \"imagery\" : { \"process_id\" : \"filter_daterange\" , \"imagery\" : { \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"extent\" : [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B4\" , \"nir\" : \"B8\" } }, \"type\" : \"WMS\" , \"enabled\" : true , \"parameters\" : { \"version\" : \"1.1.1\" }, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/services/wms-a3cca9 OpenEO-Identifier : wms-a3cca9","title":"5. Create a WMS service"},{"location":"examples-poc/#6-requesting-the-service-information","text":"Request POST https://openeo.org/api/v0.3/services/wms-a3cca9 HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"service_id\" : \"wms-a3cca9\" , \"title\" : \"Min. NDVI for Sentinel 2\" , \"description\" : \"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\" , \"process_graph\" : { \"process_id\" : \"min_time\" , \"imagery\" : { \"process_id\" : \"NDVI\" , \"imagery\" : { \"process_id\" : \"filter_daterange\" , \"imagery\" : { \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"extent\" : [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B4\" , \"nir\" : \"B8\" } }, \"url\" : \"https://openeo.org/wms/a3cca9\" , \"type\" : \"WMS\" , \"enabled\" : true , \"parameters\" : { \"version\" : \"1.1.1\" }, \"attributes\" : { \"layers\" : [ \"min_time\" ] }, \"submitted\" : \"2017-01-01T09:32:12Z\" , \"plan\" : \"free\" , \"budget\" : null }","title":"6. Requesting the service information"},{"location":"examples-poc/#7-download-the-data-on-demand-from-the-wms","text":"Omitted, not part of the openEO API. WMS is located at https://openeo.org/wms/a3cca9 .","title":"7. Download the data on demand from the WMS"},{"location":"examples-poc/#use-case-2","text":"Create a monthly aggregated Sentinel 1 product from a custom Python script.","title":"Use Case 2"},{"location":"examples-poc/#1-ask-the-back-end-for-available-sentinel-1-data","text":"Request GET /collections/Sentinel-1 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"name\" : \"Sentinel-1\" , \"description\" : \"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\" , \"license\" : \"proprietary\" , \"keywords\" : [ \"copernicus\" , \"esa\" , \"sentinel\" , \"sar\" ], \"provider\" : [ { \"name\" : \"European Space Agency (ESA)\" , \"url\" : \"https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar\" } ], \"extent\" : { \"spatial\" : [ -34 , 35 , 39 , 71 ], \"temporal\" : [ \"2016-01-01T00:00:00Z\" , null ] }, \"links\" : [ { \"rel\" : \"self\" , \"href\" : \"https://openeo.org/api/collections/Sentinel-1\" }, { \"rel\" : \"license\" , \"href\" : \"https://scihub.copernicus.eu/twiki/pub/SciHubWebPortal/TermsConditions/Sentinel_Data_Terms_and_Conditions.pdf\" } ], \"eo:constellation\" : \"sentinel-1\" , \"eo:bands\" : { \"VV\" : { \"common_name\" : \"VV\" }, \"VH\" : { \"common_name\" : \"VH\" } } }","title":"1. Ask the back-end for available Sentinel 1 data"},{"location":"examples-poc/#2-upload-python-script","text":"Request PUT /files/john_doe/s1_aggregate.py HTTP / 1.1 <File content> Response HTTP / 1.1 204 No Content","title":"2. Upload python script"},{"location":"examples-poc/#3-create-a-job","text":"Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" :{ \"process_id\" : \"aggregate_time\" , \"script\" : \"/files/john_doe/s1_aggregate.py\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-1\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] } }, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/jobs/132 OpenEO-Identifier : 132","title":"3. Create a job"},{"location":"examples-poc/#4-start-batch-processing-the-job","text":"Request POST /jobs/132/results HTTP / 1.1 Response HTTP / 1.1 202 Accepted","title":"4. Start batch processing the job"},{"location":"examples-poc/#5-create-a-tms-service","text":"Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"process_id\" : \"get_results\" , \"job_id\" : \"132\" }, \"type\" : \"TMS\" , \"enabled\" : true , \"parameters\" : {}, \"plan\" : \"free\" } Response HTTP / 1.1 201 Created Location : https://openeo.org/api/v0.3/services/tms-75ff8c","title":"5. Create a TMS service"},{"location":"examples-poc/#6-requesting-the-service-information_1","text":"Request POST https://openeo.org/api/v0.3/services/tms-75ff8c HTTP / 1.1 Content-Type : application/json; charset=utf-8 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"service_id\" : \"tms-75ff8c\" , \"title\" : \"Monthly aggregation on Sentinel 1\" , \"description\" : \"Create a monthly aggregated Sentinel 1 product from a custom Python script.\" , \"process_graph\" : { \"process_id\" : \"get_results\" , \"job_id\" : \"132\" }, \"url\" : \"https://openeo.org/tms/75ff8c\" , \"type\" : \"TMS\" , \"enabled\" : true , \"parameters\" : {}, \"attributes\" : {}, \"submitted\" : \"2018-01-01T12:32:12Z\" , \"plan\" : \"free\" , \"budget\" : null }","title":"6. Requesting the service information"},{"location":"examples-poc/#7-download-the-data-on-demand-from-the-wms_1","text":"Omitted, not part of the openEO API. TMS is located at https://openeo.org/tms/75ff8c .","title":"7. Download the data on demand from the WMS"},{"location":"examples-poc/#use-case-3","text":"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.","title":"Use Case 3"},{"location":"examples-poc/#1-upload-a-geojson-polygon","text":"Request PUT /files/john_doe/polygon1.geojson HTTP / 1.1 <File content> Response HTTP / 1.1 204 No Content","title":"1. Upload a GeoJSON Polygon"},{"location":"examples-poc/#2-create-a-job","text":"Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"bands\" : \"B8\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"regions\" : \"/files/john_doe/polygon1.geojson\" , \"func\" : \"mean\" }, \"output\" :{ \"format\" : \"GPKG\" }, \"plan\" : \"free\" , \"budget\" : null } Response HTTP / 1.1 201 Created Location : https://openeo.org/jobs/133 OpenEO-Identifier : 133","title":"2. Create a job"},{"location":"examples-poc/#3-start-batch-processing-the-job","text":"Request POST /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 202 Accepted","title":"3. Start batch processing the job"},{"location":"examples-poc/#4-check-job-status","text":"Request GET /jobs/133 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"job_id\" : \"133\" , \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel-2A\" }, \"bands\" : \"B8\" }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"regions\" : \"/files/john_doe/polygon1.geojson\" , \"func\" : \"mean\" }, \"output\" :{ \"format\" : \"GPKG\" }, \"status\" : \"finished\" , \"submitted\" : \"2017-02-01T09:32:12Z\" , \"updated\" : \"2017-02-01T09:36:18Z\" , \"plan\" : \"free\" , \"costs\" : 0 , \"budget\" : null }","title":"4. Check job status"},{"location":"examples-poc/#5-retrieve-download-links","text":"Request GET /jobs/133/results HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 { \"title\" : \"Zonal Statistics / Sentinel 2\" , \"description\" : \"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons.\" , \"updated\" : \"2017-02-01T09:36:18Z\" , \"links\" : [ { \"href\" : \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.gpkg\" , \"type\" : \"application/geopackage+sqlite3\" } ] }","title":"5. Retrieve download links"},{"location":"examples-poc/#6-download-files","text":"Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/result.gpkg HTTP / 1.1 Response A GeoPackage file, content omitted.","title":"6. Download file(s)"},{"location":"gettingstarted-backends/","text":"Getting started for back-end providers As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . If you are using Python or NodeJS to implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons NodeJS Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Capabilities: GET / and GET /output_formats Data discovery: GET /collections and GET /collections/{name} Process discovery: GET /processes Data processing: POST /preview Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Back-end Providers"},{"location":"gettingstarted-backends/#getting-started-for-back-end-providers","text":"As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . If you are using Python or NodeJS to implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons NodeJS Driver Commons You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Capabilities: GET / and GET /output_formats Data discovery: GET /collections and GET /collections/{name} Process discovery: GET /processes Data processing: POST /preview Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Getting started for back-end providers"},{"location":"gettingstarted-clients/","text":"Getting started for client developers For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines . Client library developers If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing. Applications and Software plugins Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Client Developers"},{"location":"gettingstarted-clients/#getting-started-for-client-developers","text":"For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines .","title":"Getting started for client developers"},{"location":"gettingstarted-clients/#client-library-developers","text":"If your preferred programming language is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing.","title":"Client library developers"},{"location":"gettingstarted-clients/#applications-and-software-plugins","text":"Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Applications and Software plugins"},{"location":"gettingstarted-users/","text":"Getting started for users Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries. Contribute Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Users"},{"location":"gettingstarted-users/#getting-started-for-users","text":"Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries.","title":"Getting started for users"},{"location":"gettingstarted-users/#contribute","text":"Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Contribute"},{"location":"glossary/","text":"Glossary This glossary introduces the major technical terms used in the openEO project. General terms API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data User defined functions (UDFs) : concept, that enables the users to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on UDFs for more information. EO data In our domain, different terms are used to describe EO data(sets). Within openEO, a granule typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites) while a collection is an aggregation of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists synonyms used (by organizations) for: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA) Processes and process graphs The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects. Aggregation and resampling Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells), and an aggregation function (e.g., mean ) that computes one or more new values from the original ones. Examples: a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (grouping predicate: full time extent) a time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells) Note that for the first example, the aggregation function not only requires time series values, but also their time stamps. Resampling (also called scaling ) is a broader term where we have data at one resolution, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). Resampling from finer to coarser grid may again be a special case of aggregation. When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here. User-defined functions The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.","title":"Glossary"},{"location":"glossary/#glossary","text":"This glossary introduces the major technical terms used in the openEO project.","title":"Glossary"},{"location":"glossary/#general-terms","text":"API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end : server infrastructure where industry and researchers analyse large amounts of EO data User defined functions (UDFs) : concept, that enables the users to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on UDFs for more information.","title":"General terms"},{"location":"glossary/#eo-data","text":"In our domain, different terms are used to describe EO data(sets). Within openEO, a granule typically refers to a limited area and a single overpass leading to a very short observation period (seconds) or a temporal aggregation of such data (e.g. for 16-day MODIS composites) while a collection is an aggregation of granules sharing the same product specification. It typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation. The CEOS OpenSearch Best Practice Document v1.2 lists synonyms used (by organizations) for: granule : dataset (ESA, ISO 19115), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ESA, ISO 19115), collection (CNES, NASA), dataset (JAXA), product (JAXA)","title":"EO data"},{"location":"glossary/#processes-and-process-graphs","text":"The terms process and process graph have specific meanings in the openEO API specification. A process is an operation provided by the back end that performs a specific task on a set of parameters and returns a result. An example is computing a statistical operation, such as mean or median, on selected EO data. A process is similar to a function or method in programming languages. A process graph chains specific process calls. Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects.","title":"Processes and process graphs"},{"location":"glossary/#aggregation-and-resampling","text":"Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells), and an aggregation function (e.g., mean ) that computes one or more new values from the original ones. Examples: a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (grouping predicate: full time extent) a time series may be aggregated to monthly values by computing the mean for all values in a month (grouping predicate: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (grouping predicate: 100 m x 100 m grid cells) Note that for the first example, the aggregation function not only requires time series values, but also their time stamps. Resampling (also called scaling ) is a broader term where we have data at one resolution, and need values at another. In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be simply assigned to the nearest high resolution grid cells (nearest neighbor method), or may be interpolated using various methods (e.g. by bilinear interpolation). Resampling from finer to coarser grid may again be a special case of aggregation. When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are similar, (e.g. the source collection provides 10 day intervals and the target needs values for 16 day intervals), then some form of interpolation may be more appropriate than aggregation as defined here.","title":"Aggregation and resampling"},{"location":"glossary/#user-defined-functions","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.","title":"User-defined functions"},{"location":"guidelines-api/","text":"API Development Guidelines To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive. Language Language Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 . Casing Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 . Technical requirements HTTP The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS. Verbs Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE). If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body. Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property. Resource naming Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it. Cross-Origin Resource Sharing (CORS) All back-end providers SHOULD support CORS. More information can be found in the corresponding section . Status codes and error handling The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . More information can be found in the section about status und error handling . Requests and response formats JSON Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using Content Negotiation . Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order. Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources. Temporal data Date, time, intervals and durations MUST be formatted based on ISO 8601 or any profile ( RFC 3339 is strongly recommended) if there is an appropriate encoding available in the standard. All temporal data MUST by specified based on the Gregorian calendar.","title":"API Specification"},{"location":"guidelines-api/#api-development-guidelines","text":"To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.","title":"API Development Guidelines"},{"location":"guidelines-api/#language","text":"","title":"Language"},{"location":"guidelines-api/#language_1","text":"Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 .","title":"Language"},{"location":"guidelines-api/#casing","text":"Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 .","title":"Casing"},{"location":"guidelines-api/#technical-requirements","text":"","title":"Technical requirements"},{"location":"guidelines-api/#http","text":"The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS.","title":"HTTP"},{"location":"guidelines-api/#verbs","text":"Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE). If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body. Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.","title":"Verbs"},{"location":"guidelines-api/#resource-naming","text":"Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.","title":"Resource naming"},{"location":"guidelines-api/#cross-origin-resource-sharing-cors","text":"All back-end providers SHOULD support CORS. More information can be found in the corresponding section .","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"guidelines-api/#status-codes-and-error-handling","text":"The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . More information can be found in the section about status und error handling .","title":"Status codes and error handling"},{"location":"guidelines-api/#requests-and-response-formats","text":"","title":"Requests and response formats"},{"location":"guidelines-api/#json","text":"Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using Content Negotiation . Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order. Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.","title":"JSON"},{"location":"guidelines-api/#temporal-data","text":"Date, time, intervals and durations MUST be formatted based on ISO 8601 or any profile ( RFC 3339 is strongly recommended) if there is an appropriate encoding available in the standard. All temporal data MUST by specified based on the Gregorian calendar.","title":"Temporal data"},{"location":"guidelines-clients/","text":"Client library development guidelines This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods. Conventions Casing Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters. Scopes Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the version method in openEO : Procedural style: openeo_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . version (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_get_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . getCapabilities (); Scope categories Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories, as is the case with version() (relates to (1) the client library version in openEO scope and (2) the API version in Connection scope). Parameters The parameters usually follow the request schemas in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement for languages that support named parameters such as R. Other languages may have problems implementing this natively as they need to fill many parameters with default values. For example creating a job in R with a budget would lead to such a method call: createJob(process_graph = ..., null, budget = 123) , but in PHP it would be: createJob(..., null, null, null, null, null, 123) . This is not an ideal behaviour, therefore client developers might want to consider passing parameters in coupled in a dictionary or class to emulate named parameters. The example in PHP could be improved to createJob([process_graph => ..., null, budget => 123]) . ToDo: Allow sorting and other useful operations for lists? Method mappings Note: Subscriptions and some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional. Scope: openEO (root category) Description Client method Connect to a back-end, including authentication. Returns Connection . connect(url, ?auth_type, ?auth_options) Get client library version. version() Parameters auth_type in connect : null , basic or oidc (non-exclusive). Defaults to null (no authentication). auth_options in connect : May hold additional data for authentication, for example a username and password for basic authentication. Scope: Connection (API category) Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /output_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{name} describeCollection(name) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files/{user_id} listFiles(?user_id) Creates a (virtual) file. Returns a File . None createFile(path, ?user_id) Validates a process graph. POST /validate validateProcessGraph(process_graph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(process_graph, ?title, ?description) Executes a process graph synchronously. POST /preview execute(process_graph, ?output_format, ?output_parameters, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(process_graph, ?output_format, ?output_parameters, ?title, ?description, ?plan, ?budget, ?additional) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(process_graph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Parameters user_id in listFiles and createFile : Defaults to the user id of the authenticated user. options in authenticateOIDC : May hold additional data required for OpenID connect authentication. Scope Capabilities (Content category) Should be prefixed with Capabilities if required. In non-object-oriented paradigms it is likely required as version() in this scope and the scope OpenEO could collide. For example, version() in this scope could be named openeo_capabilities_version() in procedural style. Description Field Client method Get openEO version. version version() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(method_name) Get default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans() Parameters method_name in hasFeature : The name of a client method in any of the scopes that are part of the API category. E.g. hasFeature(\"describeAccount\") checks whether the GET /me endpoint is contained in the capabilities response's endpoints object. Scope: File (API category) The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{user_id}/{path} downloadFile(target) Upload a user file. PUT /files/{user_id}/{path} uploadFile(source) Delete a user file. DELETE /files/{user_id}/{path} deleteFile() Parameters target in downloadFile : Path to a local file or folder. Scope: Job (API category) The Job scope internally knows the job_id . Description API Request Client method Get all job information. GET /jobs/{job_id} describeJob() Update a job. PATCH /jobs/{job_id} updateJob(?process_graph, ?output_format, ?output_parameters, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults(?type) Download job results. GET /jobs/{job_id}/results > ... downloadResults(target) Parameters type in listResult : Either json or metalink (non-exclusive). Defaults to json . target in downloadResults : Path to a local folder. Scope: ProcessGraph (API category) The ProcessGraph scope internally knows the pg_id ( process_graph_id ). Description API Request Client method Get all information about a stored process graph. GET /process_graphs/{pg_id} describeProcessGraph() Update a stored process graph. PATCH /process_graphs/{pg_id} updateProcessGraph(?process_graph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{pg_id} deleteProcessGraph() Scope: Service (API category) The Service scope internally knows the service_id . Description API Request Client method Get all information about a secondary web service. GET /services/{service_id} describeService() Update a secondary web service. PATCH /services/{service_id} updateService(?process_graph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService() Processes The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible. Workflow example Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ . R (functional style) library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% version () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"get_collection\" , name = \"Sentinel-2A\" ) %>% process ( \"filterBbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filterDaterange\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"NDVI\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"minTime\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" ) Python (mixed style) import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print cap . version () print con . describe_collection ( \"Sentinel-2A\" ) print con . list_processes () processes = con . get_processes (); pg = processes . get_collection ( name = \"Sentinel-2A\" ); pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_daterange ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . NDVI ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" ) Java (object oriented style) import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . version ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" ); PHP (procedural style) require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_capabilities_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"get_collection\", [\"name\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_daterange\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"NDVI\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"Client Library Development"},{"location":"guidelines-clients/#client-library-development-guidelines","text":"This is a proposal for workflows that client libraries should support to make the experience with each library similar and users can easily adopt examples and workflows. For best experience libraries should still embrace best practices common in their environments. This means clients can... choose which kind of casing they use (see below). feel free to implement aliases for methods.","title":"Client library development guidelines"},{"location":"guidelines-clients/#conventions","text":"","title":"Conventions"},{"location":"guidelines-clients/#casing","text":"Clients can use snake_case , camelCase or any method used commonly in their environment. For example, the API request to get a list of collections can either be names get_collections or getCollections . This applies for all names, including scopes, method names and parameters.","title":"Casing"},{"location":"guidelines-clients/#scopes","text":"Each method belongs to a scope. To achieve this in object-oriented (OO) programming languages, methods would be part of a class. If programming languages don't support scopes, you may need to simulate it somehow to prevent name collisions, e.g. by adding a prefix to the method names (like in the \"procedural style\" example below). Best practices for this will likely evolve over time. Example for the version method in openEO : Procedural style: openeo_version() Object-oriented style: OpenEO obj = new OpenEO (); obj . version (); If you can't store scope data in an object, you may need to pass these information as argument(s) to the method. Example: Procedural style: $connection = openeo_connect(\"https://openeo.org\"); openeo_get_capabilities($connection); Object-oriented style: OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" ); con . getCapabilities ();","title":"Scopes"},{"location":"guidelines-clients/#scope-categories","text":"Each scope is assigned to a scope category, of which there are three: Root category: Contains only the scope openEO . API category: Mostly methods hiding API calls to the back-ends. Methods may be implemented asynchronously. Contains the scopes Connection , File , Job , ProcessGraph , Service . Content : Mostly methods hiding the complexity of response content. Methods are usually implemented synchronously. Currently contains only the scope Capabilities . Method names should be prefixed if name collisions are likely. Method names across ALL the scopes that belong to the root or API categories MUST be unique. This is the case because the parameter in hasFeature(method_name) must be unambiguous. Method names of scopes in the Content category may collide with method names of scopes in the root / API categories, as is the case with version() (relates to (1) the client library version in openEO scope and (2) the API version in Connection scope).","title":"Scope categories"},{"location":"guidelines-clients/#parameters","text":"The parameters usually follow the request schemas in the openAPI specification. The parameters should follow their characteristics, for example regarding the default values. Some methods have a long list of (optional) parameters. This is easy to implement for languages that support named parameters such as R. Other languages may have problems implementing this natively as they need to fill many parameters with default values. For example creating a job in R with a budget would lead to such a method call: createJob(process_graph = ..., null, budget = 123) , but in PHP it would be: createJob(..., null, null, null, null, null, 123) . This is not an ideal behaviour, therefore client developers might want to consider passing parameters in coupled in a dictionary or class to emulate named parameters. The example in PHP could be improved to createJob([process_graph => ..., null, budget => 123]) . ToDo: Allow sorting and other useful operations for lists?","title":"Parameters"},{"location":"guidelines-clients/#method-mappings","text":"Note: Subscriptions and some scopes for response JSON objects are still missing. We are open for proposals. Parameters with a leading ? are optional.","title":"Method mappings"},{"location":"guidelines-clients/#scope-openeo-root-category","text":"Description Client method Connect to a back-end, including authentication. Returns Connection . connect(url, ?auth_type, ?auth_options) Get client library version. version()","title":"Scope: openEO (root category)"},{"location":"guidelines-clients/#parameters_1","text":"auth_type in connect : null , basic or oidc (non-exclusive). Defaults to null (no authentication). auth_options in connect : May hold additional data for authentication, for example a username and password for basic authentication.","title":"Parameters"},{"location":"guidelines-clients/#scope-connection-api-category","text":"Description API Request Client method Get the capabilities of the back-end. Returns Capabilities . GET / capabilities() List the supported output file formats. GET /output_formats listFileTypes() List the supported secondary service types. GET /service_types listServiceTypes() List all collections available on the back-end. GET /collections listCollections() Get information about a single collection. GET /collections/{name} describeCollection(name) List all processes available on the back-end. GET /processes listProcesses() Authenticate with OpenID Connect (if not specified in connect ). GET /credentials/oidc authenticateOIDC(?options) Authenticate with HTTP Basic (if not specified in connect ). GET /credentials/basic authenticateBasic(username, password) Get information about the authenticated user. GET /me describeAccount() Lists all files from a user. Returns a list of File . GET /files/{user_id} listFiles(?user_id) Creates a (virtual) file. Returns a File . None createFile(path, ?user_id) Validates a process graph. POST /validate validateProcessGraph(process_graph) Lists all process graphs of the authenticated user. Returns a list of ProcessGraph . GET /process_graphs listProcessGraphs() Creates a new stored process graph. Returns a ProcessGraph . POST /process_graphs createProcessGraph(process_graph, ?title, ?description) Executes a process graph synchronously. POST /preview execute(process_graph, ?output_format, ?output_parameters, ?budget) Lists all jobs of the authenticated user. Returns a list of Job . GET /jobs listJobs() Creates a new job. Returns a Job . POST /jobs createJob(process_graph, ?output_format, ?output_parameters, ?title, ?description, ?plan, ?budget, ?additional) Lists all secondary services of the authenticated user. Returns a list of Service . GET /services listServices() Creates a new secondary service. Returns a Service . POST /services createService(process_graph, type, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget)","title":"Scope: Connection (API category)"},{"location":"guidelines-clients/#parameters_2","text":"user_id in listFiles and createFile : Defaults to the user id of the authenticated user. options in authenticateOIDC : May hold additional data required for OpenID connect authentication.","title":"Parameters"},{"location":"guidelines-clients/#scope-capabilities-content-category","text":"Should be prefixed with Capabilities if required. In non-object-oriented paradigms it is likely required as version() in this scope and the scope OpenEO could collide. For example, version() in this scope could be named openeo_capabilities_version() in procedural style. Description Field Client method Get openEO version. version version() List all supported features / endpoints. endpoints listFeatures() Check whether a feature / endpoint is supported. endpoints > ... hasFeature(method_name) Get default billing currency. billing > currency currency() List all billing plans. billing > plans listPlans()","title":"Scope Capabilities (Content category)"},{"location":"guidelines-clients/#parameters_3","text":"method_name in hasFeature : The name of a client method in any of the scopes that are part of the API category. E.g. hasFeature(\"describeAccount\") checks whether the GET /me endpoint is contained in the capabilities response's endpoints object.","title":"Parameters"},{"location":"guidelines-clients/#scope-file-api-category","text":"The File scope internally knows the user_id and the path . Description API Request Client method Download a user file. GET /files/{user_id}/{path} downloadFile(target) Upload a user file. PUT /files/{user_id}/{path} uploadFile(source) Delete a user file. DELETE /files/{user_id}/{path} deleteFile()","title":"Scope: File (API category)"},{"location":"guidelines-clients/#parameters_4","text":"target in downloadFile : Path to a local file or folder.","title":"Parameters"},{"location":"guidelines-clients/#scope-job-api-category","text":"The Job scope internally knows the job_id . Description API Request Client method Get all job information. GET /jobs/{job_id} describeJob() Update a job. PATCH /jobs/{job_id} updateJob(?process_graph, ?output_format, ?output_parameters, ?title, ?description, ?plan, ?budget, ?additional) Delete a job DELETE /jobs/{job_id} deleteJob() Calculate an time/cost estimate for a job. GET /jobs/{job_id}/estimate estimateJob() Start / queue a job for processing. POST /jobs/{job_id}/results startJob() Stop / cancel job processing. DELETE /jobs/{job_id}/results stopJob() Get document with download links. GET /jobs/{job_id}/results listResults(?type) Download job results. GET /jobs/{job_id}/results > ... downloadResults(target)","title":"Scope: Job (API category)"},{"location":"guidelines-clients/#parameters_5","text":"type in listResult : Either json or metalink (non-exclusive). Defaults to json . target in downloadResults : Path to a local folder.","title":"Parameters"},{"location":"guidelines-clients/#scope-processgraph-api-category","text":"The ProcessGraph scope internally knows the pg_id ( process_graph_id ). Description API Request Client method Get all information about a stored process graph. GET /process_graphs/{pg_id} describeProcessGraph() Update a stored process graph. PATCH /process_graphs/{pg_id} updateProcessGraph(?process_graph, ?title, ?description) Delete a stored process graph. DELETE /process_graphs/{pg_id} deleteProcessGraph()","title":"Scope: ProcessGraph (API category)"},{"location":"guidelines-clients/#scope-service-api-category","text":"The Service scope internally knows the service_id . Description API Request Client method Get all information about a secondary web service. GET /services/{service_id} describeService() Update a secondary web service. PATCH /services/{service_id} updateService(?process_graph, ?title, ?description, ?enabled, ?parameters, ?plan, ?budget) Delete a secondary web service. DELETE /services/{service_id} deleteService()","title":"Scope: Service (API category)"},{"location":"guidelines-clients/#processes","text":"The processes a back-end supports may be offered by the clients as methods in its own scope. The method names should follow the process names, but the conventions listed above can be applied here as well, e.g. converting filter_bands to filterBands . As parameters have no natural or technical ordering in the JSON objects, clients must come up with a reasonable ordering of parameters if required. This could be inspired by existing clients. The way of building a process graph from processes heavily depends on the technical capabilities of the programming language. Therefore it may differ between the client libraries. Follow the best practices of the programming language, e.g. support method chaining if possible.","title":"Processes"},{"location":"guidelines-clients/#workflow-example","text":"Some simplified example workflows using different programming styles are listed below. The following steps are executed: Loading the client library. Connecting to a back-end and authenticating with username and password via OpenID Connect. Requesting the capabilities and showing the implemented openEO version of the back-end. Showing information about the \"Sentinel-2A\" collection. Showing information about all processes supported by the back-end. Building a simple process graph. Creating a job. Pushing the job to the processing queue. After a while, showing the job details, e.g. checking the job status. Once processing is finished, downloading the job results to the local directory /tmp/job_results/ .","title":"Workflow example"},{"location":"guidelines-clients/#r-functional-style","text":"library ( openeo ) con = connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = capabilities () cap %>% version () con %>% describeCollection ( \"Sentinel-2A\" ) con %>% listProcesses () processgraph = process ( \"get_collection\" , name = \"Sentinel-2A\" ) %>% process ( \"filterBbox\" , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) %>% process ( \"filterDaterange\" , extent = c ( \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" )) %>% process ( \"NDVI\" , nir = \"B4\" , red = \"B8A\" ) %>% process ( \"minTime\" ) job = con %>% createJob ( processgraph ) job %>% startJob () job %>% describeJob () job %>% downloadResults ( \"/tmp/job_results/\" )","title":"R (functional style)"},{"location":"guidelines-clients/#python-mixed-style","text":"import openeo con = openeo . connect ( \"https://openeo.org\" , \"username\" , \"password\" ) cap = con . capabilities () print cap . version () print con . describe_collection ( \"Sentinel-2A\" ) print con . list_processes () processes = con . get_processes (); pg = processes . get_collection ( name = \"Sentinel-2A\" ); pg = processes . filter_bbox ( pg , west = 672000 , south = 5181000 , east = 652000 , north = 5161000 , crs = \"EPSG:32632\" ) pg = processes . filter_daterange ( pg , extent = [ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ]) pg = processes . NDVI ( pg , nir = \"B4\" , red = \"B8A\" ) pg = processes . min_time ( pg ) job = con . create_job ( pg . graph ) job . start_job () print job . describe_job () job . download_results ( \"/tmp/job_results/\" )","title":"Python (mixed style)"},{"location":"guidelines-clients/#java-object-oriented-style","text":"import org.openeo.* ; OpenEO obj = new OpenEO (); Connection con = obj . connect ( \"https://openeo.org\" , \"username\" , \"password\" ); Capabilities cap = con . capabilities (); System . out . println ( cap . version ()); System . out . println ( con . describeCollection ( \"Sentinel-2A\" )); System . out . println ( con . listProcesses ()); ProcessGraphBuilder pgb = con . getProcessGraphBuilder () // Chain processes... ProcessGraph processGraph = pgb . buildProcessGraph (); Job job = con . createJob ( processGraph ); job . startJob (); System . out . println ( job . describeJob ()); job . downloadResults ( \"/tmp/job_results/\" );","title":"Java (object oriented style)"},{"location":"guidelines-clients/#php-procedural-style","text":"require_once(\"/path/to/openeo.php\"); $connection = openeo_connect(\"http://openeo.org\", \"username\", \"password\"); $capabilities = openeo_capabilities($connection); echo openeo_capabilities_version($capabilites); echo openeo_describe_collection($connection, \"Sentinel-2A\"); echo openeo_list_processes($connection); $pg = openeo_process($pg, \"get_collection\", [\"name\" => \"Sentinel-2A\"]); $pg = openeo_process($pg, \"filter_bbox\", [\"west\" => 672000, \"south\" => 5181000, \"east\" => 652000, \"north\" => 5161000, \"crs\" => \"EPSG:32632\"]); $pg = openeo_process($pg, \"filter_daterange\", [\"extent\" => [\"2017-01-01T00:00:00Z\", \"2017-01-31T23:59:59Z\"]]); $pg = openeo_process($pg, \"NDVI\", [\"red\" => \"B4\", \"nir\" => \"B8A\"]); $pg = openeo_process($pg, \"min_time\"); $job = openeo_create_job($connection, $pg); openeo_start_job($job); echo openeo_describe_job($job); openeo_download_results($job, \"/tmp/job_results/\");","title":"PHP (procedural style)"},{"location":"guidelines-software/","text":"Software Development Guidelines This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 . Software quality guidelines software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided Software documentation guidelines Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used. Software review sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken Test-driven development Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework. Continuous integration Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested. Additional guidelines There a specific guidelines for client library development and API development .","title":"Software Development"},{"location":"guidelines-software/#software-development-guidelines","text":"This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 .","title":"Software Development Guidelines"},{"location":"guidelines-software/#software-quality-guidelines","text":"software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided","title":"Software quality guidelines"},{"location":"guidelines-software/#software-documentation-guidelines","text":"Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used.","title":"Software documentation guidelines"},{"location":"guidelines-software/#software-review","text":"sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken","title":"Software review"},{"location":"guidelines-software/#test-driven-development","text":"Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.","title":"Test-driven development"},{"location":"guidelines-software/#continuous-integration","text":"Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested.","title":"Continuous integration"},{"location":"guidelines-software/#additional-guidelines","text":"There a specific guidelines for client library development and API development .","title":"Additional guidelines"},{"location":"jobs/","text":"Processing data using a process graph Process graphs can be executed in three different ways. Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back office's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible. Another way of processing and accessing data are secondary web services . They allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run on demand , i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user. Process graphs can also be executed synchronously ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests . Data processing details Heterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes. Temporal references are always specified on the basis of the Gregorian calendar . Examples Synchronously executed jobs Retrieval of a GeoTIFF Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"NDVI\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"S2_L2A_T32TPS_20M\" }, \"extent\" :{ \"west\" : 672000 , \"south\" : 5181000 , \"east\" : 652000 , \"north\" : 5161000 , \"crs\" : \"EPSG:32632\" } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } }, \"output\" :{ \"format\" : \"GTiff\" , \"args\" :{ \"tiled\" : true , \"compress\" : \"jpeg\" , \"photometric\" : \"YCBCR\" , \"jpeg_quality\" : 80 } } } Response HTTP / 1.1 200 OK Content-Type : image/tiff Access-Control-Allow-Origin : <Origin> omitted (the GeoTiff file contents) Retrieval of time series Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : 8 }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , null ] }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/octet-stream Access-Control-Allow-Origin : <Origin> omitted (the GeoPackage file contents)","title":"Jobs"},{"location":"jobs/#processing-data-using-a-process-graph","text":"Process graphs can be executed in three different ways. Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back office's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible. Another way of processing and accessing data are secondary web services . They allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run on demand , i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user. Process graphs can also be executed synchronously ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests .","title":"Processing data using a process graph"},{"location":"jobs/#data-processing-details","text":"Heterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes. Temporal references are always specified on the basis of the Gregorian calendar .","title":"Data processing details"},{"location":"jobs/#examples","text":"","title":"Examples"},{"location":"jobs/#synchronously-executed-jobs","text":"","title":"Synchronously executed jobs"},{"location":"jobs/#retrieval-of-a-geotiff","text":"Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"NDVI\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"S2_L2A_T32TPS_20M\" }, \"extent\" :{ \"west\" : 672000 , \"south\" : 5181000 , \"east\" : 652000 , \"north\" : 5161000 , \"crs\" : \"EPSG:32632\" } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } }, \"output\" :{ \"format\" : \"GTiff\" , \"args\" :{ \"tiled\" : true , \"compress\" : \"jpeg\" , \"photometric\" : \"YCBCR\" , \"jpeg_quality\" : 80 } } } Response HTTP / 1.1 200 OK Content-Type : image/tiff Access-Control-Allow-Origin : <Origin> omitted (the GeoTiff file contents)","title":"Retrieval of a GeoTIFF"},{"location":"jobs/#retrieval-of-time-series","text":"Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : 8 }, \"extent\" :{ \"west\" : 16.1 , \"south\" : 47.2 , \"east\" : 16.6 , \"north\" : 48.6 } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , null ] }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/octet-stream Access-Control-Allow-Origin : <Origin> omitted (the GeoPackage file contents)","title":"Retrieval of time series"},{"location":"processes/","text":"Processes A process is an operation that performs a specific task, see the glossary for a detailed definition. It consists of a name, a set of parameters, a return type and may throw errors or exceptions. In openEO, processes are used to build a chain of processes ( process graph ), which can be applied to EO data to derive your own findings from the data. Core processes There are some processes that we define to be core processes that are pre-defined and back-ends SHOULD follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes. Note Currently, there are only few defined processes. Those are only meant as an example how future documentation of processes may look like until the core processes get defined in version 0.4. Defining processes Any back-end provider can either implement a set of pre-defined processes (STRONGLY RECOMMENDED) or define new processes for their domain. To define new processes, back-end providers should know: The name is the identifier for the process and MUST never contain a forward slash / . Each parameter has a name and the content follows a schema. The content returned by a process also follows a schema. The schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below. openEO specific formats In addition to the native data formats specified by JSON schema, openEO defines a set of specific formats that should be re-used in process schema definitions: Format Name Description eodata A proprietary way to pass the processed data from one process to another. temporal_extent A temporal extent as formally specified by the openEO API. spatial_extent A spatial extent (with crs) as formally specified by the openEO API. Note These formats are still evolving and will likely change with the definition of the openEO processes for openEO API version 0.4.","title":"Processes"},{"location":"processes/#processes","text":"A process is an operation that performs a specific task, see the glossary for a detailed definition. It consists of a name, a set of parameters, a return type and may throw errors or exceptions. In openEO, processes are used to build a chain of processes ( process graph ), which can be applied to EO data to derive your own findings from the data.","title":"Processes"},{"location":"processes/#core-processes","text":"There are some processes that we define to be core processes that are pre-defined and back-ends SHOULD follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes. Note Currently, there are only few defined processes. Those are only meant as an example how future documentation of processes may look like until the core processes get defined in version 0.4.","title":"Core processes"},{"location":"processes/#defining-processes","text":"Any back-end provider can either implement a set of pre-defined processes (STRONGLY RECOMMENDED) or define new processes for their domain. To define new processes, back-end providers should know: The name is the identifier for the process and MUST never contain a forward slash / . Each parameter has a name and the content follows a schema. The content returned by a process also follows a schema. The schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below.","title":"Defining processes"},{"location":"processes/#openeo-specific-formats","text":"In addition to the native data formats specified by JSON schema, openEO defines a set of specific formats that should be re-used in process schema definitions: Format Name Description eodata A proprietary way to pass the processed data from one process to another. temporal_extent A temporal extent as formally specified by the openEO API. spatial_extent A spatial extent (with crs) as formally specified by the openEO API. Note These formats are still evolving and will likely change with the definition of the openEO processes for openEO API version 0.4.","title":"openEO specific formats"},{"location":"processgraphs/","text":"Process graphs A process graph is a chain of specific processes . Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects. Schematic definition A process graph is defined to consist of chained processes: <ProcessGraph> := <Process> An argument value of a process can hold a Process again. This allows chaining of processes. Process A single process in a process graph is defined as follows: <Process> := { \"process_id\": <string>, \"process_description\": <string>, \"<ArgumentName>\": <Value>, ... } A process MUST always contain a key-value-pair named process_id and MAY contain a process_description . It MAY hold an arbitrary number of additional elements as arguments for the process. process_id can contain any of the processes defined by a back-end, which are all listed at GET /processes , e.g. get_collection to retrieve data from a specific collection for processing. Arguments A process can have an arbitrary number of arguments. The key <ArgumentName> can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . <ArgumentName> MUST NOT use the names process_id or process_description as it would result in a naming conflict. A value is defined as follows: <Value> := <string|number|boolean|null|array|object|Process|Variable> Note \u200b The specified data types except Process and Variable (see definition above) are the native data types supported by JSON. Limitations apply: \u200b * Objects are not allowed to have keys with the following names: \u200b * process_id , except for objects of type Process \u200b * variable_id , except for objects of type Variable Caution \u200b The expected names of arguments are defined by the process descriptions, which can be discovered with calls to GET /processes . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using collection as a key once. Variables Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Variable> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <Value> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined, the default value is used. <Value> can be used as defined above, but MUST NOT be a Variable . Values for variables can be specified in the query string or body of endpoints supporting variables. See the API reference for more information. Examples Example 1: A full process graph definition including a variable for the collection name . { \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"/udf/Python/custom_ndvi\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" :{ \"variable_id\" : \"product\" , \"description\" : \"Identifier of the collection\" , \"type\" : \"string\" , \"default\" : \"S2_L2A_T32TPS_20M\" } }, \"extent\" :{ \"west\" : 652000 , \"south\" : 5181000 , \"east\" : 672000 , \"north\" : 5161000 , \"crs\" : \"EPSG:32632\" } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } Example 2: If a process needs multiple processes as input, it is allowed to use arrays of the respective types. { \"imagery\" :{ \"process_id\" : \"union\" , \"collection\" :[ { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : \"8\" }, { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : \"5\" } ] } }","title":"Process Graphs"},{"location":"processgraphs/#process-graphs","text":"A process graph is a chain of specific processes . Similarly to scripts in the context of programming, process graphs organize and automate the execution of one or more processes that could alternatively be executed individually. In a process graph, processes need to be specific, i.e. concrete values for input parameters need to be specified. These arguments can again be process graphs, scalar values, arrays or objects.","title":"Process graphs"},{"location":"processgraphs/#schematic-definition","text":"A process graph is defined to consist of chained processes: <ProcessGraph> := <Process> An argument value of a process can hold a Process again. This allows chaining of processes.","title":"Schematic definition"},{"location":"processgraphs/#process","text":"A single process in a process graph is defined as follows: <Process> := { \"process_id\": <string>, \"process_description\": <string>, \"<ArgumentName>\": <Value>, ... } A process MUST always contain a key-value-pair named process_id and MAY contain a process_description . It MAY hold an arbitrary number of additional elements as arguments for the process. process_id can contain any of the processes defined by a back-end, which are all listed at GET /processes , e.g. get_collection to retrieve data from a specific collection for processing.","title":"Process"},{"location":"processgraphs/#arguments","text":"A process can have an arbitrary number of arguments. The key <ArgumentName> can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . <ArgumentName> MUST NOT use the names process_id or process_description as it would result in a naming conflict. A value is defined as follows: <Value> := <string|number|boolean|null|array|object|Process|Variable> Note \u200b The specified data types except Process and Variable (see definition above) are the native data types supported by JSON. Limitations apply: \u200b * Objects are not allowed to have keys with the following names: \u200b * process_id , except for objects of type Process \u200b * variable_id , except for objects of type Variable Caution \u200b The expected names of arguments are defined by the process descriptions, which can be discovered with calls to GET /processes . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using collection as a key once.","title":"Arguments"},{"location":"processgraphs/#variables","text":"Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Variable> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <Value> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined, the default value is used. <Value> can be used as defined above, but MUST NOT be a Variable . Values for variables can be specified in the query string or body of endpoints supporting variables. See the API reference for more information.","title":"Variables"},{"location":"processgraphs/#examples","text":"Example 1: A full process graph definition including a variable for the collection name . { \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"/udf/Python/custom_ndvi\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" :{ \"variable_id\" : \"product\" , \"description\" : \"Identifier of the collection\" , \"type\" : \"string\" , \"default\" : \"S2_L2A_T32TPS_20M\" } }, \"extent\" :{ \"west\" : 652000 , \"south\" : 5181000 , \"east\" : 672000 , \"north\" : 5161000 , \"crs\" : \"EPSG:32632\" } }, \"extent\" :[ \"2017-01-01T00:00:00Z\" , \"2017-01-31T23:59:59Z\" ] }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } Example 2: If a process needs multiple processes as input, it is allowed to use arrays of the respective types. { \"imagery\" :{ \"process_id\" : \"union\" , \"collection\" :[ { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : \"8\" }, { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_collection\" , \"name\" : \"Sentinel2-L1C\" }, \"bands\" : \"5\" } ] } }","title":"Examples"},{"location":"processreference/","text":"Placeholder for generated process specifications.","title":"Process Reference"},{"location":"udfs/","text":"User-defined functions The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"UDFs"},{"location":"udfs/#user-defined-functions","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. UDFs are currently developed and evaluated outside of the core API. More information regarding the current draft for UDFs can be found in a separate repository . There is additional documentation available for the UDF Framework and the UDF API .","title":"User-defined functions"},{"location":"usermanagement/","text":"User Management and Accounting In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"},{"location":"usermanagement/#user-management-and-accounting","text":"In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"}]}